import { NextRequest } from 'next/server'
import { AI_B_CONFIG, generateEnhancedSystemPrompt, callAIStreaming } from '@/lib/ai-config'
import { cacheManager } from '@/lib/cache-manager'

export async function POST(request: NextRequest) {
  try {
    const { question, aiAResponse, round, fullDiscussion } = await request.json()
    
    // æ™ºèƒ½ç¼“å­˜æ£€æŸ¥
    const cacheKey = `discuss:${question}:${round}:${aiAResponse.slice(0, 50)}`
    const cachedResult = cacheManager.get<string>(cacheKey)
    
    if (cachedResult) {
      // è¿”å›ç¼“å­˜çš„è®¨è®ºå†…å®¹
      const encoder = new TextEncoder()
      const stream = new ReadableStream({
        start(controller) {
          const chunks = cachedResult.split(' ')
          let index = 0
          
          const sendChunk = () => {
            if (index < chunks.length) {
              const chunk = chunks[index] + (index < chunks.length - 1 ? ' ' : '')
              const data = `data: ${JSON.stringify({ content: chunk, cached: true })}\n\n`
              controller.enqueue(encoder.encode(data))
              index++
              setTimeout(sendChunk, 25) // å¿«é€Ÿè¿”å›ç¼“å­˜å†…å®¹
            } else {
              controller.enqueue(encoder.encode('data: [DONE]\n\n'))
              controller.close()
            }
          }
          
          sendChunk()
        }
      })
      
      return new Response(stream, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
        }
      })
    }
    
    // ğŸ§  ç®—æ³•ä¼˜åŒ–2: åˆ†æå¯¹è¯è´¨é‡å’Œè¶‹åŠ¿
    const dialogueQuality = analyzeDialogueQuality(fullDiscussion, round)
    
    // æ ¹æ®å¯¹è¯è´¨é‡è°ƒæ•´ç³»ç»Ÿæç¤ºè¯
    const systemPrompt = generateEnhancedSystemPrompt(AI_B_CONFIG, 'ai_b', round, question)
    
    let enhancedPrompt = systemPrompt
    if (dialogueQuality.needsRefocus) {
      enhancedPrompt += `

ğŸ¯ å¯¹è¯ä¼˜åŒ–æç¤ºï¼šæ£€æµ‹åˆ°å¯¹è¯å¯èƒ½åç¦»ä¸»é¢˜ï¼Œè¯·é‡æ–°èšç„¦æ ¸å¿ƒé—®é¢˜ã€‚
å½“å‰å¯¹è¯è´¨é‡ï¼š${(dialogueQuality.score * 100).toFixed(1)}%
å»ºè®®ï¼š${dialogueQuality.suggestion}`
    }
    
    const userPrompt = `ç”¨æˆ·é—®é¢˜ï¼š"${question}"

AIåŠ©æ‰‹Açš„è§‚ç‚¹ï¼š
${aiAResponse}

${fullDiscussion ? `\nå®Œæ•´è®¨è®ºå†å²ï¼š\n${fullDiscussion}\n` : ''}

ğŸ¤– å¯¹è¯åˆ†æï¼š
- å½“å‰è½®æ¬¡ï¼š${round}
- å¯¹è¯è´¨é‡ï¼š${(dialogueQuality.score * 100).toFixed(1)}%
- æ”¶æ•›è¶‹åŠ¿ï¼š${dialogueQuality.trend}
- å»ºè®®ç­–ç•¥ï¼š${dialogueQuality.suggestion}

è¯·å›åº”AIåŠ©æ‰‹Açš„è§‚ç‚¹ï¼Œç»§ç»­è®¨è®ºã€‚`
    
    const encoder = new TextEncoder()
    let fullResponse = ''
    
    const stream = new ReadableStream({
      async start(controller) {
        try {
          // å‘é€å¯¹è¯åˆ†æç»“æœ
          const analysisData = `data: ${JSON.stringify({ 
            dialogueAnalysis: dialogueQuality,
            message: `ğŸ¤– å¯¹è¯è´¨é‡åˆ†æï¼š${(dialogueQuality.score * 100).toFixed(1)}%` 
          })}\n\n`
          controller.enqueue(encoder.encode(analysisData))
          
          await callAIStreaming(AI_B_CONFIG, enhancedPrompt, userPrompt, (chunk) => {
            fullResponse += chunk
            const data = `data: ${JSON.stringify({ content: chunk })}\n\n`
            controller.enqueue(encoder.encode(data))
          })
          
          // æ™ºèƒ½ç¼“å­˜å­˜å‚¨
          if (fullResponse.length > 50) {
            cacheManager.set(cacheKey, fullResponse)
          }
          
          controller.enqueue(encoder.encode('data: [DONE]\n\n'))
        } catch (error: any) {
          const errorData = `data: ${JSON.stringify({ error: error.message })}\n\n`
          controller.enqueue(encoder.encode(errorData))
        } finally {
          controller.close()
        }
      }
    })

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      }
    })
  } catch (error: any) {
    return new Response(JSON.stringify({ error: error.message }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' }
    })
  }
}

// ğŸ§  å¯¹è¯è´¨é‡åˆ†æç®—æ³•
function analyzeDialogueQuality(fullDiscussion: string, round: number) {
  if (!fullDiscussion) {
    return {
      score: 0.8,
      trend: 'starting',
      needsRefocus: false,
      suggestion: 'å¼€å§‹æ–°çš„è®¨è®º'
    }
  }
  
  const responses = fullDiscussion.split('\n\n').filter(r => r.trim().length > 0)
  
  // è®¡ç®—å¯¹è¯è´¨é‡åˆ†æ•°
  let qualityScore = 0.5
  
  // é•¿åº¦è´¨é‡
  const avgLength = responses.reduce((sum, r) => sum + r.length, 0) / responses.length
  if (avgLength > 200) qualityScore += 0.2
  if (avgLength > 500) qualityScore += 0.1
  
  // ä¸“ä¸šæœ¯è¯­å¯†åº¦
  const technicalTerms = ['ç®—æ³•', 'ç³»ç»Ÿ', 'æ¶æ„', 'è®¾è®¡', 'å®ç°', 'ä¼˜åŒ–', 'æ€§èƒ½']
  const techDensity = responses.reduce((sum, r) => {
    const techCount = technicalTerms.filter(term => r.includes(term)).length
    return sum + techCount
  }, 0) / responses.length
  
  if (techDensity > 1) qualityScore += 0.1
  if (techDensity > 2) qualityScore += 0.1
  
  // äº’åŠ¨è´¨é‡
  const interactionWords = ['æˆ‘åŒæ„', 'æˆ‘è®¤ä¸º', 'ä½ æåˆ°çš„', 'è®©æˆ‘ä»¬', 'è¿›ä¸€æ­¥', 'å¦å¤–']
  const interactionCount = responses.reduce((sum, r) => {
    return sum + interactionWords.filter(word => r.includes(word)).length
  }, 0)
  
  if (interactionCount > responses.length * 0.5) qualityScore += 0.1
  
  // åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°èšç„¦
  const needsRefocus = round > 3 && qualityScore < 0.6
  
  // è¶‹åŠ¿åˆ†æ
  let trend = 'stable'
  if (round > 2) {
    if (qualityScore > 0.8) trend = 'improving'
    else if (qualityScore < 0.5) trend = 'declining'
  }
  
  // å»ºè®®ç­–ç•¥
  let suggestion = 'ç»§ç»­å½“å‰è®¨è®ºæ–¹å‘'
  if (needsRefocus) {
    suggestion = 'å»ºè®®å›åˆ°æ ¸å¿ƒé—®é¢˜ï¼Œæ·±å…¥åˆ†æ'
  } else if (qualityScore > 0.8) {
    suggestion = 'è®¨è®ºè´¨é‡è‰¯å¥½ï¼Œå¯ä»¥æ·±å…¥ç»†èŠ‚'
  } else if (round > 4 && qualityScore > 0.7) {
    suggestion = 'å¯ä»¥è€ƒè™‘æ€»ç»“å…±è¯†'
  }
  
  return {
    score: Math.min(qualityScore, 1.0),
    trend,
    needsRefocus,
    suggestion,
    round,
    technicalDensity: techDensity,
    interactionLevel: interactionCount / responses.length
  }
}
