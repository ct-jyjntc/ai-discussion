import { NextRequest } from 'next/server'
import { AI_A_CONFIG, generateSystemPrompt, callAIStreaming } from '@/lib/ai-config'
import { cacheManager } from '@/lib/cache-manager'

export async function POST(request: NextRequest) {
  try {
    const { question, round } = await request.json()
    
    // æ™ºèƒ½ç¼“å­˜æ£€æŸ¥
    const cacheKey = `analyze:${question}:${round}`
    const cachedResult = cacheManager.get<string>(cacheKey)
    
    if (cachedResult) {
      // è¿”å›ç¼“å­˜ç»“æœï¼Œæ˜¾è‘—æå‡å“åº”é€Ÿåº¦
      const encoder = new TextEncoder()
      const stream = new ReadableStream({
        start(controller) {
          // æ¨¡æ‹Ÿæµå¼è¿”å›ç¼“å­˜å†…å®¹
          const chunks = cachedResult.split(' ')
          let index = 0
          
          const sendChunk = () => {
            if (index < chunks.length) {
              const chunk = chunks[index] + (index < chunks.length - 1 ? ' ' : '')
              const data = `data: ${JSON.stringify({ content: chunk, cached: true })}\n\n`
              controller.enqueue(encoder.encode(data))
              index++
              setTimeout(sendChunk, 30) // å¿«é€Ÿè¿”å›ç¼“å­˜å†…å®¹
            } else {
              controller.enqueue(encoder.encode('data: [DONE]\n\n'))
              controller.close()
            }
          }
          
          sendChunk()
        }
      })
      
      return new Response(stream, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
        }
      })
    }

    // ç®€åŒ–çš„é—®é¢˜åˆ†æ
    const questionAnalysis = {
      type: question.includes('å¦‚ä½•') || question.includes('æ€ä¹ˆ') ? 'practical' : 'general',
      complexity: question.length > 50 ? 0.8 : 0.5,
      specificityLevel: question.split('?').length > 2 ? 'high' : 'medium'
    }
    
    // æ ¹æ®é—®é¢˜å¤æ‚åº¦è°ƒæ•´ç³»ç»Ÿæç¤ºè¯
    let enhancedSystemPrompt = generateSystemPrompt(AI_A_CONFIG, 'ai_a', round)
    
    if (questionAnalysis.complexity > 0.7) {
      enhancedSystemPrompt += `

ğŸ§  æ™ºèƒ½åˆ†ææç¤ºï¼šæ­¤é—®é¢˜å…·æœ‰è¾ƒé«˜å¤æ‚åº¦ï¼Œå»ºè®®æ·±åº¦åˆ†æã€‚
- é—®é¢˜ç±»å‹ï¼š${questionAnalysis.type}
- å¤æ‚åº¦è¯„ä¼°ï¼š${(questionAnalysis.complexity * 100).toFixed(1)}%
- ä¸“ä¸šåº¦ï¼š${questionAnalysis.specificityLevel}`
    }
    
    const userPrompt = `ç”¨æˆ·é—®é¢˜ï¼š"${question}"

ğŸ” æ™ºèƒ½é—®é¢˜åˆ†æï¼š
- é—®é¢˜ç±»å‹ï¼š${questionAnalysis.type}
- å¤æ‚åº¦è¯„ä¼°ï¼š${(questionAnalysis.complexity * 100).toFixed(1)}%
- ä¸“ä¸šç¨‹åº¦ï¼š${questionAnalysis.specificityLevel}

è¯·å¼€å§‹åˆ†æè¿™ä¸ªé—®é¢˜ï¼Œæå‡ºä½ çš„åˆæ­¥è§‚ç‚¹ã€‚`
    
    const encoder = new TextEncoder()
    let fullResponse = ''
    
    const stream = new ReadableStream({
      async start(controller) {
        try {
          // å‘é€é—®é¢˜åˆ†æç»“æœ
          if (round === 1) {
            const analysisData = `data: ${JSON.stringify({ 
              analysis: questionAnalysis,
              message: 'ğŸ§  æ™ºèƒ½é—®é¢˜åˆ†æå®Œæˆ' 
            })}\n\n`
            controller.enqueue(encoder.encode(analysisData))
          }
          
          await callAIStreaming(AI_A_CONFIG, enhancedSystemPrompt, userPrompt, (chunk) => {
            fullResponse += chunk
            const data = `data: ${JSON.stringify({ content: chunk })}\n\n`
            controller.enqueue(encoder.encode(data))
          })
          
          // æ™ºèƒ½ç¼“å­˜å­˜å‚¨
          if (fullResponse.length > 50) {
            cacheManager.set(cacheKey, fullResponse)
            
            // å‘é€ç¼“å­˜ç»Ÿè®¡
            const stats = cacheManager.getStats()
            const statsData = `data: ${JSON.stringify({ 
              cacheStats: stats,
              message: `ğŸš€ ç¼“å­˜å‘½ä¸­ç‡: ${stats.hitRate.toFixed(1)}%` 
            })}\n\n`
            controller.enqueue(encoder.encode(statsData))
          }
          
          controller.enqueue(encoder.encode('data: [DONE]\n\n'))
        } catch (error: any) {
          const errorData = `data: ${JSON.stringify({ error: error.message })}\n\n`
          controller.enqueue(encoder.encode(errorData))
        } finally {
          controller.close()
        }
      }
    })

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      }
    })
  } catch (error: any) {
    return new Response(JSON.stringify({ error: error.message }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' }
    })
  }
}
